{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "# Double Q-Learning Training Notebook\n",
    "\n",
    "This notebook demonstrates training a Double Q-Learning agent for autonomous parcel routing.\n",
    "\n",
    "## Key Features:\n",
    "- **Reduced Overestimation Bias**: Uses two separate Q-tables\n",
    "- **Better Convergence**: More stable learning compared to standard Q-Learning\n",
    "- **Action Selection**: Combines both Q-tables for decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "from apr import WarehouseEnv\n",
    "from apr.agents import create_agent\n",
    "from apr.train import run_episode\n",
    "from apr.logger import RunLogger\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Environment and Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = WarehouseEnv(seed=42)\n",
    "print(f\"Environment: {env.n_rows}x{env.n_cols} warehouse\")\n",
    "print(f\"Packages to collect: {len(env.packages_remaining)}\")\n",
    "print(f\"Max steps per episode: {env.max_steps}\")\n",
    "\n",
    "# Visualize initial environment\n",
    "env.reset()\n",
    "env.render(mode='human')\n",
    "plt.title('Initial Warehouse Layout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Double Q-Learning agent\n",
    "agent = create_agent(\n",
    "    'double_q_learning',\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    alpha=0.1,          # Learning rate\n",
    "    gamma=0.95,         # Discount factor\n",
    "    epsilon=0.3,        # Initial exploration rate\n",
    "    epsilon_decay=0.999, # Exploration decay\n",
    "    epsilon_min=0.05    # Minimum exploration\n",
    ")\n",
    "\n",
    "print(f\"Created agent: {agent}\")\n",
    "print(f\"Agent has dual Q-tables: Q1={hasattr(agent, 'Q1')}, Q2={hasattr(agent, 'Q2')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header-2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "episodes = 1000\n",
    "log_interval = 100\n",
    "\n",
    "# Metrics tracking\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "epsilon_values = []\n",
    "success_rates = []\n",
    "\n",
    "print(\"ðŸš€ Starting Double Q-Learning Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # Run episode\n",
    "    reward = run_episode(env, agent, training=True)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(reward)\n",
    "    episode_lengths.append(env.episode_length if hasattr(env, 'episode_length') else 0)\n",
    "    epsilon_values.append(agent.epsilon)\n",
    "    \n",
    "    # Calculate success rate (rolling window)\n",
    "    if episode >= 100:\n",
    "        recent_rewards = episode_rewards[-100:]\n",
    "        success_count = sum(1 for r in recent_rewards if r > 400)  # Successful episodes\n",
    "        success_rates.append(success_count / 100)\n",
    "    else:\n",
    "        success_rates.append(0)\n",
    "    \n",
    "    # Logging\n",
    "    if (episode + 1) % log_interval == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-log_interval:])\n",
    "        print(f\"Episode {episode + 1:4d}: Avg Reward = {avg_reward:6.1f}, Îµ = {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.1f}\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Learning curve (smoothed)\n",
    "ax1 = axes[0, 0]\n",
    "window = 50\n",
    "smoothed_rewards = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), smoothed_rewards, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward (smoothed)')\n",
    "ax1.set_title('Double Q-Learning: Learning Curve')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Epsilon decay\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epsilon_values, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "ax2.set_title('Exploration Rate Decay')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Success rate over time\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(success_rates, 'g-', linewidth=2)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Success Rate (100-episode window)')\n",
    "ax3.set_title('Success Rate Evolution')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Reward distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(episode_rewards[-200:], bins=30, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(np.mean(episode_rewards[-200:]), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(episode_rewards[-200:]):.1f}')\n",
    "ax4.set_xlabel('Reward')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Final Reward Distribution (Last 200 Episodes)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qtable-analysis",
   "metadata": {},
   "source": [
    "## Q-Table Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-qtables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-tables\n",
    "print(\"ðŸ“Š Double Q-Learning Q-Table Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get Q-table statistics\n",
    "if hasattr(agent, 'get_q_statistics'):\n",
    "    stats = agent.get_q_statistics()\n",
    "    \n",
    "    print(f\"Number of states visited: {stats['num_states']}\")\n",
    "    print(f\"State space coverage: {stats['num_states']/(env.n_rows*env.n_cols):.1%}\")\n",
    "    \n",
    "    print(\"\\nQ1 Table Statistics:\")\n",
    "    q1_stats = stats['q1_stats']\n",
    "    print(f\"  Mean: {q1_stats['mean']:.2f}\")\n",
    "    print(f\"  Std:  {q1_stats['std']:.2f}\")\n",
    "    print(f\"  Range: [{q1_stats['min']:.2f}, {q1_stats['max']:.2f}]\")\n",
    "    \n",
    "    print(\"\\nQ2 Table Statistics:\")\n",
    "    q2_stats = stats['q2_stats']\n",
    "    print(f\"  Mean: {q2_stats['mean']:.2f}\")\n",
    "    print(f\"  Std:  {q2_stats['std']:.2f}\")\n",
    "    print(f\"  Range: [{q2_stats['min']:.2f}, {q2_stats['max']:.2f}]\")\n",
    "    \n",
    "    print(\"\\nCombined Q-Table Statistics:\")\n",
    "    combined_stats = stats['combined_stats']\n",
    "    print(f\"  Mean: {combined_stats['mean']:.2f}\")\n",
    "    print(f\"  Std:  {combined_stats['std']:.2f}\")\n",
    "    print(f\"  Range: [{combined_stats['min']:.2f}, {combined_stats['max']:.2f}]\")\n",
    "\n",
    "# Visualize Q-value distributions\n",
    "if hasattr(agent, 'Q1') and hasattr(agent, 'Q2'):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Collect Q-values\n",
    "    q1_values = []\n",
    "    q2_values = []\n",
    "    combined_values = []\n",
    "    \n",
    "    for state in agent.Q1.keys():\n",
    "        q1_values.extend(agent.Q1[state])\n",
    "        q2_values.extend(agent.Q2[state])\n",
    "        combined_values.extend(agent.Q[state])\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[0].hist(q1_values, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0].set_title('Q1 Value Distribution')\n",
    "    axes[0].set_xlabel('Q-Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[1].hist(q2_values, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[1].set_title('Q2 Value Distribution')\n",
    "    axes[1].set_xlabel('Q-Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[2].hist(combined_values, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[2].set_title('Combined Q-Value Distribution')\n",
    "    axes[2].set_xlabel('Q-Value')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent",
   "metadata": {},
   "source": [
    "## Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Testing Trained Double Q-Learning Agent\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test agent performance (no exploration)\n",
    "test_episodes = 50\n",
    "test_rewards = []\n",
    "successful_episodes = 0\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < env.max_steps:\n",
    "        state = env.agent_pos\n",
    "        action = agent.act(state, training=False)  # No exploration\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    test_rewards.append(total_reward)\n",
    "    if total_reward > 400:  # Successful episode threshold\n",
    "        successful_episodes += 1\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Episodes: {test_episodes}\")\n",
    "print(f\"Mean Reward: {np.mean(test_rewards):.1f} Â± {np.std(test_rewards):.1f}\")\n",
    "print(f\"Success Rate: {successful_episodes/test_episodes:.1%}\")\n",
    "print(f\"Best Performance: {np.max(test_rewards):.1f}\")\n",
    "print(f\"Worst Performance: {np.min(test_rewards):.1f}\")\n",
    "\n",
    "# Visualize test performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(test_rewards, 'o-', alpha=0.7)\n",
    "plt.axhline(np.mean(test_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(test_rewards):.1f}')\n",
    "plt.xlabel('Test Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Test Episode Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_rewards, bins=15, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(test_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(test_rewards):.1f}')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Test Reward Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-episode",
   "metadata": {},
   "source": [
    "## Demonstration Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¬ Demonstration Episode\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Reset environment for demo\n",
    "env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "trajectory = []\n",
    "\n",
    "print(f\"Initial state: Agent at {env.agent_pos}\")\n",
    "print(f\"Packages to collect: {len(env.packages_remaining)}\")\n",
    "print(f\"Dropoff location: {env.dropoff}\")\n",
    "print()\n",
    "\n",
    "# Run demonstration episode\n",
    "while not done and steps < 20:  # Limit to 20 steps for demo\n",
    "    state = env.agent_pos\n",
    "    action = agent.act(state, training=False)\n",
    "    \n",
    "    # Action names for readability\n",
    "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    trajectory.append((state, action, reward, next_state))\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {steps + 1}: {state} â†’ {action_names[action]} â†’ {next_state} (reward: {reward:+.0f})\")\n",
    "    \n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nDemo completed after {steps} steps\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Packages remaining: {len(env.packages_remaining)}\")\n",
    "print(f\"Carrying packages: {env.carrying_packages}\")\n",
    "\n",
    "# Render final state\n",
    "env.render(mode='human')\n",
    "plt.title(f'Agent State After {steps} Steps (Reward: {total_reward})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-agent",
   "metadata": {},
   "source": [
    "## Save Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save directory\n",
    "save_dir = Path('../models')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save agent\n",
    "agent_path = save_dir / 'double_q_learning_trained.pkl'\n",
    "agent.save(agent_path)\n",
    "\n",
    "print(f\"âœ… Agent saved to: {agent_path}\")\n",
    "print(f\"Final performance: {np.mean(episode_rewards[-100:]):.1f} average reward (last 100 episodes)\")\n",
    "print(f\"Training episodes: {episodes}\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'episode': range(1, episodes + 1),\n",
    "    'reward': episode_rewards,\n",
    "    'epsilon': epsilon_values,\n",
    "    'success_rate': success_rates\n",
    "})\n",
    "\n",
    "metrics_path = save_dir / 'double_q_learning_metrics.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f\"ðŸ“Š Training metrics saved to: {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}