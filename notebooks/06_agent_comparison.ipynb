{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "# Agent Comparison Notebook\n",
    "\n",
    "This notebook provides comprehensive comparison of all implemented RL algorithms for autonomous parcel routing.\n",
    "\n",
    "## Algorithms Compared:\n",
    "1. **Q-Learning**: Classic off-policy temporal difference learning\n",
    "2. **Double Q-Learning**: Reduces overestimation bias with dual Q-tables\n",
    "3. **SARSA**: On-policy temporal difference learning\n",
    "4. **SARSA(Œª)**: SARSA with eligibility traces\n",
    "\n",
    "## Comparison Metrics:\n",
    "- Learning performance and convergence\n",
    "- Final policy quality\n",
    "- Sample efficiency\n",
    "- Exploration behavior\n",
    "- Robustness across different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "from apr import WarehouseEnv, AgentEvaluator\n",
    "from apr.agents import create_agent\n",
    "from apr.train import run_episode\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Ensure reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Environment and Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = WarehouseEnv(seed=42)\n",
    "print(f\"Environment: {env.n_rows}x{env.n_cols} warehouse\")\n",
    "print(f\"Packages to collect: {len(env.packages_remaining)}\")\n",
    "print(f\"Max steps per episode: {env.max_steps}\")\n",
    "\n",
    "# Visualize environment\n",
    "env.reset()\n",
    "env.render(mode='human')\n",
    "plt.title('Warehouse Environment Layout')\n",
    "plt.show()\n",
    "\n",
    "# Algorithm configurations\n",
    "algorithms = {\n",
    "    'Q-Learning': {\n",
    "        'name': 'q_learning',\n",
    "        'params': {'alpha': 0.1, 'gamma': 0.95, 'epsilon': 0.3, 'epsilon_decay': 0.999},\n",
    "        'color': 'skyblue',\n",
    "        'description': 'Off-policy TD learning with experience replay'\n",
    "    },\n",
    "    'Double Q-Learning': {\n",
    "        'name': 'double_q_learning',\n",
    "        'params': {'alpha': 0.1, 'gamma': 0.95, 'epsilon': 0.3, 'epsilon_decay': 0.999},\n",
    "        'color': 'lightgreen',\n",
    "        'description': 'Dual Q-tables to reduce overestimation bias'\n",
    "    },\n",
    "    'SARSA': {\n",
    "        'name': 'sarsa',\n",
    "        'params': {'alpha': 0.1, 'gamma': 0.95, 'epsilon': 0.3, 'epsilon_decay': 0.999},\n",
    "        'color': 'lightcoral',\n",
    "        'description': 'On-policy TD learning'\n",
    "    },\n",
    "    'SARSA(Œª)': {\n",
    "        'name': 'sarsa_lambda',\n",
    "        'params': {'alpha': 0.1, 'gamma': 0.95, 'epsilon': 0.3, 'epsilon_decay': 0.999, 'lambda_': 0.9},\n",
    "        'color': 'lightyellow',\n",
    "        'description': 'SARSA with eligibility traces'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nAlgorithms to compare: {list(algorithms.keys())}\")\n",
    "for name, config in algorithms.items():\n",
    "    print(f\"  {name}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-comparison",
   "metadata": {},
   "source": [
    "## Training Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-comparison-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent_name, config, episodes=800, log_interval=100):\n",
    "    \"\"\"\n",
    "    Train an agent and return training metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\nü§ñ Training {agent_name}...\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = create_agent(\n",
    "        config['name'],\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        **config['params']\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilon_values = []\n",
    "    training_times = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        episode_start = time.time()\n",
    "        \n",
    "        # Run episode\n",
    "        reward = run_episode(env, agent, training=True)\n",
    "        episode_time = time.time() - episode_start\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(reward)\n",
    "        episode_lengths.append(env.episode_length if hasattr(env, 'episode_length') else 0)\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "        training_times.append(episode_time)\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-log_interval:])\n",
    "            print(f\"  Episode {episode + 1:3d}: Avg Reward = {avg_reward:6.1f}, Œµ = {agent.epsilon:.3f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ‚úÖ {agent_name} completed in {total_time:.1f}s\")\n",
    "    print(f\"     Final performance: {np.mean(episode_rewards[-50:]):.1f} (last 50 episodes)\")\n",
    "    \n",
    "    return {\n",
    "        'agent': agent,\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'epsilon_values': epsilon_values,\n",
    "        'training_times': training_times,\n",
    "        'total_time': total_time,\n",
    "        'final_performance': np.mean(episode_rewards[-50:])\n",
    "    }\n",
    "\n",
    "# Train all agents\n",
    "print(\"üèÅ Starting Algorithm Comparison Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "training_results = {}\n",
    "for agent_name, config in algorithms.items():\n",
    "    training_results[agent_name] = train_agent(agent_name, config, episodes=600)\n",
    "\n",
    "print(\"\\nüéâ All training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-curves",
   "metadata": {},
   "source": [
    "## Learning Curves Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-learning-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive learning curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Raw learning curves\n",
    "ax1 = axes[0, 0]\n",
    "for agent_name, results in training_results.items():\n",
    "    color = algorithms[agent_name]['color']\n",
    "    ax1.plot(results['rewards'], alpha=0.3, color=color, linewidth=0.5)\n",
    "    \n",
    "    # Add smoothed curve\n",
    "    window = 50\n",
    "    smoothed = np.convolve(results['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(results['rewards'])), smoothed, \n",
    "             color=color, linewidth=2, label=agent_name)\n",
    "\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Learning Curves (Raw + Smoothed)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Epsilon decay comparison\n",
    "ax2 = axes[0, 1]\n",
    "for agent_name, results in training_results.items():\n",
    "    color = algorithms[agent_name]['color']\n",
    "    ax2.plot(results['epsilon_values'], color=color, linewidth=2, label=agent_name)\n",
    "\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "ax2.set_title('Exploration Decay')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Final performance comparison\n",
    "ax3 = axes[1, 0]\n",
    "agent_names = list(training_results.keys())\n",
    "final_performances = [results['final_performance'] for results in training_results.values()]\n",
    "colors = [algorithms[name]['color'] for name in agent_names]\n",
    "\n",
    "bars = ax3.bar(agent_names, final_performances, color=colors, alpha=0.8)\n",
    "ax3.set_xlabel('Algorithm')\n",
    "ax3.set_ylabel('Final Performance (Last 50 Episodes)')\n",
    "ax3.set_title('Final Performance Comparison')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, perf in zip(bars, final_performances):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'{perf:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Training time comparison\n",
    "ax4 = axes[1, 1]\n",
    "training_times = [results['total_time'] for results in training_results.values()]\n",
    "bars = ax4.bar(agent_names, training_times, color=colors, alpha=0.8)\n",
    "ax4.set_xlabel('Algorithm')\n",
    "ax4.set_ylabel('Training Time (seconds)')\n",
    "ax4.set_title('Training Efficiency')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars, training_times):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convergence-analysis",
   "metadata": {},
   "source": [
    "## Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convergence-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence characteristics\n",
    "def analyze_convergence(rewards, window=100):\n",
    "    \"\"\"\n",
    "    Analyze convergence characteristics of learning curve.\n",
    "    \"\"\"\n",
    "    if len(rewards) < window * 2:\n",
    "        return {'error': 'Insufficient data'}\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    # Find convergence point (when improvement becomes < 5% over 100 episodes)\n",
    "    convergence_episode = len(moving_avg)\n",
    "    for i in range(window, len(moving_avg) - window):\n",
    "        current_avg = np.mean(moving_avg[i:i+window])\n",
    "        previous_avg = np.mean(moving_avg[i-window:i])\n",
    "        improvement = (current_avg - previous_avg) / abs(previous_avg) if previous_avg != 0 else 0\n",
    "        \n",
    "        if improvement < 0.05:  # Less than 5% improvement\n",
    "            convergence_episode = i\n",
    "            break\n",
    "    \n",
    "    # Sample efficiency (episodes to reach 80% of final performance)\n",
    "    final_perf = np.mean(rewards[-50:])\n",
    "    target_perf = 0.8 * final_perf\n",
    "    sample_efficiency = len(rewards)\n",
    "    \n",
    "    for i, avg in enumerate(moving_avg):\n",
    "        if avg >= target_perf:\n",
    "            sample_efficiency = i + window\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'convergence_episode': convergence_episode,\n",
    "        'sample_efficiency': sample_efficiency,\n",
    "        'final_performance': final_perf,\n",
    "        'stability': np.std(rewards[-100:])  # Stability in final 100 episodes\n",
    "    }\n",
    "\n",
    "# Analyze all algorithms\n",
    "convergence_analysis = {}\n",
    "for agent_name, results in training_results.items():\n",
    "    convergence_analysis[agent_name] = analyze_convergence(results['rewards'])\n",
    "\n",
    "# Create convergence comparison table\n",
    "convergence_data = []\n",
    "for agent_name, analysis in convergence_analysis.items():\n",
    "    if 'error' not in analysis:\n",
    "        convergence_data.append({\n",
    "            'Algorithm': agent_name,\n",
    "            'Convergence Episode': analysis['convergence_episode'],\n",
    "            'Sample Efficiency': analysis['sample_efficiency'],\n",
    "            'Final Performance': f\"{analysis['final_performance']:.1f}\",\n",
    "            'Stability (œÉ)': f\"{analysis['stability']:.1f}\"\n",
    "        })\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_data)\n",
    "print(\"üìä CONVERGENCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(convergence_df.to_string(index=False))\n",
    "\n",
    "# Visualize convergence metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['convergence_episode', 'sample_efficiency', 'stability']\n",
    "titles = ['Episodes to Convergence', 'Sample Efficiency', 'Final Stability']\n",
    "colors = [algorithms[name]['color'] for name in convergence_analysis.keys()]\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    values = [analysis[metric] for analysis in convergence_analysis.values() if 'error' not in analysis]\n",
    "    agent_names = [name for name, analysis in convergence_analysis.items() if 'error' not in analysis]\n",
    "    \n",
    "    bars = axes[i].bar(agent_names, values, color=colors, alpha=0.8)\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                     f'{val:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-evaluation",
   "metadata": {},
   "source": [
    "## Comprehensive Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-evaluation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the evaluation module for comprehensive testing\n",
    "print(\"üîç Comprehensive Agent Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "evaluator = AgentEvaluator(env, verbose=True)\n",
    "\n",
    "# Collect trained agents\n",
    "trained_agents = {name: results['agent'] for name, results in training_results.items()}\n",
    "\n",
    "# Run comprehensive comparison\n",
    "evaluation_results = evaluator.compare_agents(\n",
    "    trained_agents,\n",
    "    num_episodes=100,\n",
    "    seeds=[42, 123, 456]\n",
    ")\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"\\nüìä COMPREHENSIVE EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_stats = evaluation_results['comparison_stats']\n",
    "agent_stats = comparison_stats['agent_statistics']\n",
    "\n",
    "# Create detailed comparison table\n",
    "eval_data = []\n",
    "for agent_name, stats in agent_stats.items():\n",
    "    eval_data.append({\n",
    "        'Algorithm': agent_name,\n",
    "        'Mean Reward': f\"{stats['mean_reward']:.1f}\",\n",
    "        'Success Rate': f\"{stats['success_rate']:.1%}\",\n",
    "        'Episode Length': f\"{stats['episode_length']:.1f}\",\n",
    "        'State Coverage': f\"{stats['state_coverage']:.1%}\"\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(eval_df.to_string(index=False))\n",
    "\n",
    "# Print rankings\n",
    "print(\"\\nüèÜ RANKINGS:\")\n",
    "rankings = comparison_stats['rankings']\n",
    "for metric, ranking in rankings.items():\n",
    "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "    for i, (agent_name, _) in enumerate(ranking):\n",
    "        print(f\"  {i+1}. {agent_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-visualization",
   "metadata": {},
   "source": [
    "## Evaluation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-visualization-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation visualization\n",
    "evaluator.visualize_evaluation(evaluation_results)\n",
    "\n",
    "print(\"‚úÖ Comprehensive evaluation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-analysis",
   "metadata": {},
   "source": [
    "## Statistical Significance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical tests between algorithms\n",
    "from scipy import stats\n",
    "\n",
    "print(\"üìà STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Get final performance data for all algorithms\n",
    "algorithm_performances = {}\n",
    "for agent_name, results in training_results.items():\n",
    "    # Use last 100 episodes for statistical testing\n",
    "    algorithm_performances[agent_name] = results['rewards'][-100:]\n",
    "\n",
    "# Pairwise t-tests\n",
    "algorithm_names = list(algorithm_performances.keys())\n",
    "n_algorithms = len(algorithm_names)\n",
    "\n",
    "print(\"Pairwise t-test results (p-values):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create significance matrix\n",
    "significance_matrix = np.ones((n_algorithms, n_algorithms))\n",
    "\n",
    "for i in range(n_algorithms):\n",
    "    for j in range(i+1, n_algorithms):\n",
    "        alg1_name = algorithm_names[i]\n",
    "        alg2_name = algorithm_names[j]\n",
    "        \n",
    "        alg1_perf = algorithm_performances[alg1_name]\n",
    "        alg2_perf = algorithm_performances[alg2_name]\n",
    "        \n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(alg1_perf, alg2_perf)\n",
    "        significance_matrix[i, j] = p_value\n",
    "        significance_matrix[j, i] = p_value\n",
    "        \n",
    "        # Determine which is better\n",
    "        better = alg1_name if np.mean(alg1_perf) > np.mean(alg2_perf) else alg2_name\n",
    "        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "        \n",
    "        print(f\"{alg1_name} vs {alg2_name}: p={p_value:.4f} {significance} (better: {better})\")\n",
    "\n",
    "# Visualize significance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(significance_matrix))\n",
    "sns.heatmap(significance_matrix, annot=True, fmt='.4f', cmap='RdYlBu_r',\n",
    "            xticklabels=algorithm_names, yticklabels=algorithm_names,\n",
    "            mask=mask, cbar_kws={'label': 'p-value'})\n",
    "plt.title('Statistical Significance Matrix (p-values)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algorithm-characteristics",
   "metadata": {},
   "source": [
    "## Algorithm Characteristics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "algorithm-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ ALGORITHM CHARACTERISTICS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze each algorithm's strengths and weaknesses\n",
    "for agent_name, config in algorithms.items():\n",
    "    if agent_name in training_results:\n",
    "        results = training_results[agent_name]\n",
    "        convergence = convergence_analysis[agent_name]\n",
    "        \n",
    "        print(f\"\\n{agent_name}:\")\n",
    "        print(f\"  Description: {config['description']}\")\n",
    "        print(f\"  Final Performance: {results['final_performance']:.1f}\")\n",
    "        print(f\"  Training Time: {results['total_time']:.1f}s\")\n",
    "        \n",
    "        if 'error' not in convergence:\n",
    "            print(f\"  Sample Efficiency: {convergence['sample_efficiency']} episodes\")\n",
    "            print(f\"  Stability: {convergence['stability']:.1f}\")\n",
    "        \n",
    "        # Get evaluation stats if available\n",
    "        if agent_name in agent_stats:\n",
    "            eval_stats = agent_stats[agent_name]\n",
    "            print(f\"  Success Rate: {eval_stats['success_rate']:.1%}\")\n",
    "            print(f\"  State Coverage: {eval_stats['state_coverage']:.1%}\")\n",
    "\n",
    "# Overall recommendations\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Find best performer overall\n",
    "best_performer = max(training_results.items(), key=lambda x: x[1]['final_performance'])\n",
    "print(f\"‚Ä¢ Best Overall Performance: {best_performer[0]} ({best_performer[1]['final_performance']:.1f} reward)\")\n",
    "\n",
    "# Find most sample efficient\n",
    "most_efficient = min([(name, analysis) for name, analysis in convergence_analysis.items() \n",
    "                     if 'error' not in analysis], key=lambda x: x[1]['sample_efficiency'])\n",
    "print(f\"‚Ä¢ Most Sample Efficient: {most_efficient[0]} ({most_efficient[1]['sample_efficiency']} episodes)\")\n",
    "\n",
    "# Find most stable\n",
    "most_stable = min([(name, analysis) for name, analysis in convergence_analysis.items() \n",
    "                  if 'error' not in analysis], key=lambda x: x[1]['stability'])\n",
    "print(f\"‚Ä¢ Most Stable: {most_stable[0]} (œÉ = {most_stable[1]['stability']:.1f})\")\n",
    "\n",
    "print(\"\\nüî¨ INSIGHTS:\")\n",
    "print(\"-\" * 10)\n",
    "print(\"‚Ä¢ Double Q-Learning typically reduces overestimation bias\")\n",
    "print(\"‚Ä¢ SARSA is more conservative due to on-policy learning\")\n",
    "print(\"‚Ä¢ SARSA(Œª) can improve credit assignment with eligibility traces\")\n",
    "print(\"‚Ä¢ Q-Learning is often more sample efficient for deterministic environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-comparison",
   "metadata": {},
   "source": [
    "## Save Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-comparison-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive comparison results\n",
    "results_dir = Path('../comparison_results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training metrics\n",
    "training_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Algorithm': name,\n",
    "        'Final_Performance': results['final_performance'],\n",
    "        'Training_Time': results['total_time'],\n",
    "        'Convergence_Episode': convergence_analysis[name].get('convergence_episode', 'N/A'),\n",
    "        'Sample_Efficiency': convergence_analysis[name].get('sample_efficiency', 'N/A'),\n",
    "        'Stability': convergence_analysis[name].get('stability', 'N/A')\n",
    "    }\n",
    "    for name, results in training_results.items()\n",
    "])\n",
    "\n",
    "training_summary.to_csv(results_dir / 'algorithm_comparison_summary.csv', index=False)\n",
    "\n",
    "# Save detailed training curves\n",
    "for agent_name, results in training_results.items():\n",
    "    agent_df = pd.DataFrame({\n",
    "        'episode': range(1, len(results['rewards']) + 1),\n",
    "        'reward': results['rewards'],\n",
    "        'epsilon': results['epsilon_values'],\n",
    "        'training_time': results['training_times']\n",
    "    })\n",
    "    \n",
    "    filename = agent_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    agent_df.to_csv(results_dir / f'{filename}_training_curve.csv', index=False)\n",
    "\n",
    "# Save evaluation results\n",
    "if 'evaluation_results' in locals():\n",
    "    eval_df.to_csv(results_dir / 'evaluation_comparison.csv', index=False)\n",
    "\n",
    "print(f\"üíæ All comparison results saved to {results_dir}\")\n",
    "print(\"\\nüìÅ Files saved:\")\n",
    "for file in results_dir.glob('*.csv'):\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Algorithm comparison analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}