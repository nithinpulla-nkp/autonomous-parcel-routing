{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# SARSA(λ): Complete Training, Validation & Testing\n",
    "\n",
    "This notebook provides a complete workflow for SARSA(λ) agent:\n",
    "1. **Training**: Learn policy with eligibility traces\n",
    "2. **Validation**: Rigorous testing to ensure proper learning\n",
    "3. **Testing**: Evaluate final performance and behavior\n",
    "\n",
    "## SARSA(λ) Algorithm\n",
    "- **Type**: On-policy temporal difference learning with eligibility traces\n",
    "- **Update Rule**: Q(s,a) ← Q(s,a) + α δ e(s,a)\n",
    "- **Key Feature**: Eligibility traces for faster credit assignment\n",
    "- **Lambda (λ)**: Controls trace decay (0=SARSA, 1=Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "from apr import WarehouseEnv, RLAgentValidator, AgentEvaluator\n",
    "from apr.agents import create_agent\n",
    "from apr.train import run_episode\n",
    "from apr.utils import ensure_outputs_directory\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "# Ensure output directories exist\n",
    "ensure_outputs_directory()\n",
    "print(\"✅ Output directories ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Environment & Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = WarehouseEnv(seed=42)\n",
    "print(f\"Environment: {env.n_rows}x{env.n_cols} warehouse\")\n",
    "print(f\"Packages to collect: {len(env.packages_remaining)}\")\n",
    "print(f\"Max steps per episode: {env.max_steps}\")\n",
    "\n",
    "# Visualize environment\n",
    "env.reset()\n",
    "env.render(mode='human')\n",
    "plt.title('SARSA(λ) Training Environment')\n",
    "plt.show()\n",
    "\n",
    "# Create SARSA(λ) agent\n",
    "agent = create_agent(\n",
    "    'sarsa_lambda',\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    alpha=0.1,          # Learning rate\n",
    "    gamma=0.95,         # Discount factor\n",
    "    epsilon=0.3,        # Initial exploration rate\n",
    "    epsilon_decay=0.999, # Exploration decay\n",
    "    epsilon_min=0.05,   # Minimum exploration\n",
    "    lambda_=0.9         # Eligibility trace decay\n",
    ")\n",
    "\n",
    "print(f\"\\n🤖 Created SARSA(λ) Agent:\")\n",
    "print(f\"  Algorithm: {type(agent).__name__}\")\n",
    "print(f\"  Learning rate (α): {agent.alpha}\")\n",
    "print(f\"  Discount factor (γ): {agent.gamma}\")\n",
    "print(f\"  Initial exploration (ε): {agent.epsilon}\")\n",
    "print(f\"  Eligibility trace decay (λ): {agent.lambda_}\")\n",
    "print(f\"  Learning type: On-policy with eligibility traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## 2. Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_episodes = 800\n",
    "log_interval = 100\n",
    "\n",
    "print(\"🚀 Starting SARSA(λ) Training\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "epsilon_values = []\n",
    "q_table_sizes = []\n",
    "eligibility_trace_sizes = []\n",
    "training_times = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(training_episodes):\n",
    "    episode_start = time.time()\n",
    "    \n",
    "    # Run training episode\n",
    "    reward = run_episode(env, agent, training=True)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(reward)\n",
    "    episode_lengths.append(env.episode_length if hasattr(env, 'episode_length') else 0)\n",
    "    epsilon_values.append(agent.epsilon)\n",
    "    q_table_sizes.append(len(agent.Q))\n",
    "    \n",
    "    # Track eligibility traces if available\n",
    "    if hasattr(agent, 'E'):\n",
    "        active_traces = sum(1 for state_traces in agent.E.values() \n",
    "                          for trace in state_traces if trace > 0.01)\n",
    "        eligibility_trace_sizes.append(active_traces)\n",
    "    else:\n",
    "        eligibility_trace_sizes.append(0)\n",
    "    \n",
    "    training_times.append(time.time() - episode_start)\n",
    "    \n",
    "    # Logging\n",
    "    if (episode + 1) % log_interval == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-log_interval:])\n",
    "        avg_time = np.mean(training_times[-log_interval:])\n",
    "        avg_traces = np.mean(eligibility_trace_sizes[-log_interval:])\n",
    "        print(f\"Episode {episode + 1:3d}: Reward = {avg_reward:6.1f}, ε = {agent.epsilon:.3f}, \"\n",
    "              f\"Q-states = {len(agent.Q):3d}, Traces = {avg_traces:4.1f}, Time = {avg_time:.3f}s\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✅ Training Complete!\")\n",
    "print(f\"  Total time: {total_training_time:.1f}s\")\n",
    "print(f\"  Final performance: {np.mean(episode_rewards[-50:]):.1f} (last 50 episodes)\")\n",
    "print(f\"  Q-table size: {len(agent.Q)} states\")\n",
    "print(f\"  Final exploration: {agent.epsilon:.3f}\")\n",
    "print(f\"  Average active traces: {np.mean(eligibility_trace_sizes[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-analysis",
   "metadata": {},
   "source": [
    "## 3. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training analysis\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "# 1. Learning curve\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='gold', linewidth=0.5, label='Raw')\n",
    "window = 50\n",
    "smoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), smoothed, 'orange', linewidth=2, label='Smoothed')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('SARSA(λ): Learning Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Epsilon decay\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epsilon_values, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "ax2.set_title('Exploration Rate Decay')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-table growth\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(q_table_sizes, 'g-', linewidth=2)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Number of States')\n",
    "ax3.set_title('Q-Table Growth')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Eligibility traces (unique to SARSA-λ)\n",
    "ax4 = axes[1, 1]\n",
    "traces_smoothed = np.convolve(eligibility_trace_sizes, np.ones(window)//window, mode='valid')\n",
    "ax4.plot(range(window-1, len(eligibility_trace_sizes)), traces_smoothed, 'purple', linewidth=2)\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Active Eligibility Traces')\n",
    "ax4.set_title('Eligibility Traces Activity')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Episode lengths\n",
    "ax5 = axes[2, 0]\n",
    "lengths_smoothed = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "ax5.plot(range(window-1, len(episode_lengths)), lengths_smoothed, 'm-', linewidth=2)\n",
    "ax5.set_xlabel('Episode')\n",
    "ax5.set_ylabel('Episode Length')\n",
    "ax5.set_title('Episode Length Over Time')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Final reward distribution\n",
    "ax6 = axes[2, 1]\n",
    "final_rewards = episode_rewards[-200:]  # Last 200 episodes\n",
    "ax6.hist(final_rewards, bins=20, alpha=0.7, color='gold', edgecolor='black')\n",
    "ax6.axvline(np.mean(final_rewards), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(final_rewards):.1f}')\n",
    "ax6.set_xlabel('Reward')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.set_title('Final Performance Distribution')\n",
    "ax6.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print training statistics\n",
    "print(\"📊 Training Statistics:\")\n",
    "print(f\"  Episodes: {training_episodes}\")\n",
    "print(f\"  Total time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)\")\n",
    "print(f\"  Avg time per episode: {np.mean(training_times):.3f}s\")\n",
    "print(f\"  Final 100-episode average: {np.mean(episode_rewards[-100:]):.1f}\")\n",
    "print(f\"  Best single episode: {np.max(episode_rewards):.1f}\")\n",
    "print(f\"  Q-table final size: {len(agent.Q)} states\")\n",
    "print(f\"  State space coverage: {len(agent.Q)/(env.n_rows*env.n_cols)*100:.1f}%\")\n",
    "\n",
    "# SARSA(λ)-specific analysis\n",
    "print(f\"\\n🎯 SARSA(λ)-Specific Characteristics:\")\n",
    "print(f\"  Lambda (λ): {agent.lambda_}\")\n",
    "print(f\"  Eligibility traces: Enable faster credit assignment\")\n",
    "print(f\"  Average active traces: {np.mean(eligibility_trace_sizes[-100:]):.1f}\")\n",
    "print(f\"  Trace benefit: Updates multiple state-action pairs per step\")\n",
    "print(f\"  Memory: Higher than SARSA due to eligibility trace storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-section",
   "metadata": {},
   "source": [
    "## 4. Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 SARSA(λ) Agent Validation\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create fresh agent for validation (to avoid training bias)\n",
    "validation_agent = create_agent(\n",
    "    'sarsa_lambda',\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    alpha=0.1, gamma=0.95, epsilon=0.3, lambda_=0.9\n",
    ")\n",
    "\n",
    "# Run comprehensive validation\n",
    "validator = RLAgentValidator(validation_agent, env, verbose=True)\n",
    "validation_results = validator.full_validation(\n",
    "    training_episodes=300,\n",
    "    test_episodes=50,\n",
    "    n_seeds=3\n",
    ")\n",
    "\n",
    "# Display validation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = validation_results['summary']\n",
    "print(f\"Overall Assessment: {summary['overall_assessment']}\")\n",
    "print()\n",
    "\n",
    "print(\"Component Scores:\")\n",
    "for component, score in summary['scores'].items():\n",
    "    status_icon = \"✅\" if score == \"PASS\" else \"⚠️\" if score == \"WARNING\" else \"❌\"\n",
    "    print(f\"  {status_icon} {component.capitalize()}: {score}\")\n",
    "\n",
    "if summary['warnings']:\n",
    "    print(\"\\n⚠️  Warnings:\")\n",
    "    for warning in summary['warnings']:\n",
    "        print(f\"  - {warning}\")\n",
    "\n",
    "# Key metrics\n",
    "learning_result = validation_results['learning']\n",
    "exploration = validation_results['exploration']\n",
    "generalization = validation_results['generalization']\n",
    "\n",
    "print(\"\\nKey Validation Metrics:\")\n",
    "print(f\"  Learning improvement: {learning_result['improvement']:.1f} reward vs random\")\n",
    "print(f\"  Statistical significance: p={learning_result['statistical_test']['p_value']:.4f}\")\n",
    "print(f\"  State coverage: {exploration['state_coverage']['coverage_percent']:.1f}%\")\n",
    "print(f\"  Generalization consistency: {generalization['consistency_score']:.3f}\")\n",
    "\n",
    "# SARSA(λ)-specific validation notes\n",
    "print(\"\\n📝 SARSA(λ) Validation Notes:\")\n",
    "print(f\"  • Eligibility traces should improve learning speed\")\n",
    "print(f\"  • λ={agent.lambda_} provides balance between TD(0) and Monte Carlo\")\n",
    "print(f\"  • Expected to show faster convergence than standard SARSA\")\n",
    "if exploration['state_coverage']['coverage_percent'] > 50:\n",
    "    print(f\"  • Good exploration coverage suggests effective trace propagation\")\n",
    "else:\n",
    "    print(f\"  • Lower exploration typical for on-policy methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-viz",
   "metadata": {},
   "source": [
    "## 5. Validation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive validation visualization\n",
    "validator.visualize_results()\n",
    "print(\"✅ Validation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "## 6. Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 SARSA(λ) Agent Testing\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use the trained agent for testing\n",
    "evaluator = AgentEvaluator(env, verbose=True)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "test_results = evaluator.evaluate_agent(\n",
    "    agent,\n",
    "    num_episodes=100,\n",
    "    seeds=[42, 123, 456, 789, 999],  # Multiple scenarios\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Display test results\n",
    "print(\"\\n📊 TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "agg_stats = test_results['aggregated_results']['overall_statistics']\n",
    "print(f\"Mean Reward: {agg_stats['mean_reward']:.1f} ± {agg_stats['std_reward']:.1f}\")\n",
    "print(f\"Success Rate: {agg_stats['mean_success_rate']:.1%}\")\n",
    "print(f\"Mean Episode Length: {agg_stats['mean_episode_length']:.1f}\")\n",
    "print(f\"State Coverage: {agg_stats['mean_state_coverage']:.1%}\")\n",
    "\n",
    "# Performance across different seeds\n",
    "seed_results = test_results['per_seed_results']\n",
    "print(\"\\nPerformance Across Seeds:\")\n",
    "for seed_key, result in seed_results.items():\n",
    "    seed = result['seed']\n",
    "    mean_reward = result['statistics']['mean_reward']\n",
    "    success_rate = result['statistics']['success_rate']\n",
    "    print(f\"  Seed {seed}: {mean_reward:.1f} reward, {success_rate:.1%} success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-viz",
   "metadata": {},
   "source": [
    "## 7. Testing Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing visualization\n",
    "evaluator.visualize_evaluation(test_results)\n",
    "print(\"✅ Testing visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-section",
   "metadata": {},
   "source": [
    "## 8. Agent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎬 SARSA(λ) Agent Demonstration\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Reset for demonstration\n",
    "env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "max_demo_steps = 25\n",
    "\n",
    "print(f\"Initial state: Agent at {env.agent_pos}\")\n",
    "print(f\"Packages to collect: {env.packages_remaining}\")\n",
    "print(f\"Dropoff location: {env.dropoff}\")\n",
    "print()\n",
    "\n",
    "# Action names for readability\n",
    "action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "\n",
    "while not done and steps < max_demo_steps:\n",
    "    state = env.agent_pos\n",
    "    action = agent.act(state, training=False)  # No exploration\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {steps + 1:2d}: {state} → {action_names[action]:5s} → {next_state} \"\n",
    "          f\"(reward: {reward:+4.0f}, total: {total_reward:+4.0f})\")\n",
    "    \n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nDemo completed after {steps} steps\")\n",
    "print(f\"Final reward: {total_reward}\")\n",
    "print(f\"Episode completed: {done}\")\n",
    "print(f\"Packages remaining: {len(env.packages_remaining)}\")\n",
    "print(f\"Carrying packages: {env.carrying_packages}\")\n",
    "\n",
    "# Show final state\n",
    "env.render(mode='human')\n",
    "plt.title(f'SARSA(λ) Agent After {steps} Steps (Reward: {total_reward})')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 SARSA(λ) Behavior Notes:\")\n",
    "print(f\"  • Eligibility traces propagate rewards backward through trajectory\")\n",
    "print(f\"  • λ={agent.lambda_} balances immediate vs delayed credit assignment\")\n",
    "print(f\"  • Should learn faster than SARSA due to multi-step updates\")\n",
    "print(f\"  • More memory intensive due to eligibility trace storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligibility-analysis",
   "metadata": {},
   "source": [
    "## 9. Eligibility Traces Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligibility-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔥 Eligibility Traces Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Analyze eligibility traces if available\n",
    "if hasattr(agent, 'E') and len(agent.E) > 0:\n",
    "    # Extract trace information\n",
    "    all_traces = []\n",
    "    active_traces_per_state = {}\n",
    "    \n",
    "    for state, state_traces in agent.E.items():\n",
    "        if isinstance(state_traces, np.ndarray):\n",
    "            all_traces.extend(state_traces)\n",
    "            active_traces_per_state[state] = np.sum(state_traces > 0.01)\n",
    "    \n",
    "    print(f\"Eligibility Traces Statistics:\")\n",
    "    print(f\"  States with traces: {len(agent.E)}\")\n",
    "    print(f\"  Total trace values: {len(all_traces)}\")\n",
    "    print(f\"  Active traces (>0.01): {sum(1 for t in all_traces if t > 0.01)}\")\n",
    "    print(f\"  Max trace value: {np.max(all_traces):.3f}\")\n",
    "    print(f\"  Mean trace value: {np.mean(all_traces):.3f}\")\n",
    "    print(f\"  Trace decay (λ): {agent.lambda_}\")\n",
    "    \n",
    "    # Visualize eligibility traces\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 1. Trace value distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    active_traces = [t for t in all_traces if t > 0.001]\n",
    "    if active_traces:\n",
    "        plt.hist(active_traces, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "        plt.xlabel('Trace Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Active Eligibility Trace Distribution')\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    # 2. Traces per state\n",
    "    plt.subplot(1, 3, 2)\n",
    "    trace_counts = list(active_traces_per_state.values())\n",
    "    if trace_counts:\n",
    "        plt.hist(trace_counts, bins=min(10, max(trace_counts)), alpha=0.7, \n",
    "                color='orange', edgecolor='black')\n",
    "        plt.xlabel('Active Traces per State')\n",
    "        plt.ylabel('Number of States')\n",
    "        plt.title('Trace Activity per State')\n",
    "    \n",
    "    # 3. Trace activity over training\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(eligibility_trace_sizes, alpha=0.5, color='purple', linewidth=0.5)\n",
    "    trace_smoothed = np.convolve(eligibility_trace_sizes, np.ones(50)/50, mode='valid')\n",
    "    plt.plot(range(49, len(eligibility_trace_sizes)), trace_smoothed, 'purple', linewidth=2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Active Traces')\n",
    "    plt.title('Trace Activity Over Training')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📈 Trace Analysis:\")\n",
    "    print(f\"  Peak trace activity: {np.max(eligibility_trace_sizes)} active traces\")\n",
    "    print(f\"  Average activity (final 100 episodes): {np.mean(eligibility_trace_sizes[-100:]):.1f}\")\n",
    "    print(f\"  Trace efficiency: {np.mean(eligibility_trace_sizes)/len(agent.Q)*100:.1f}% of Q-states have traces\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  No eligibility traces available for analysis\")\n",
    "    print(\"    This might indicate the agent wasn't properly initialized as SARSA(λ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "policy-analysis",
   "metadata": {},
   "source": [
    "## 10. Policy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "policy-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧠 SARSA(λ) Policy Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Analyze learned Q-values and policy\n",
    "if hasattr(agent, 'Q') and len(agent.Q) > 0:\n",
    "    # Extract Q-values and policy\n",
    "    all_q_values = []\n",
    "    policy = {}\n",
    "    state_values = {}\n",
    "    \n",
    "    for state, q_vals in agent.Q.items():\n",
    "        if isinstance(q_vals, np.ndarray):\n",
    "            all_q_values.extend(q_vals)\n",
    "            policy[state] = np.argmax(q_vals)\n",
    "            state_values[state] = np.max(q_vals)\n",
    "    \n",
    "    print(f\"Q-table Statistics:\")\n",
    "    print(f\"  States learned: {len(agent.Q)}\")\n",
    "    print(f\"  Q-value range: [{np.min(all_q_values):.1f}, {np.max(all_q_values):.1f}]\")\n",
    "    print(f\"  Q-value mean: {np.mean(all_q_values):.1f}\")\n",
    "    print(f\"  Q-value std: {np.std(all_q_values):.1f}\")\n",
    "    \n",
    "    # Action distribution in policy\n",
    "    from collections import Counter\n",
    "    action_dist = Counter(policy.values())\n",
    "    print(f\"\\nPolicy Action Distribution:\")\n",
    "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "    for action, count in action_dist.items():\n",
    "        percentage = count / len(policy) * 100\n",
    "        print(f\"  {action_names[action]:5s}: {count:3d} states ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # Compare eligibility-enhanced learning\n",
    "    print(f\"\\n🎯 SARSA(λ) Policy Characteristics:\")\n",
    "    print(f\"  • Eligibility traces enable faster value propagation\")\n",
    "    print(f\"  • Should converge faster than standard SARSA\")\n",
    "    print(f\"  • λ={agent.lambda_} balances TD(0) and Monte Carlo methods\")\n",
    "    print(f\"  • More stable than Q-Learning in stochastic environments\")\n",
    "    \n",
    "    # Visualize Q-value distribution and policy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(all_q_values, bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
    "    plt.axvline(np.mean(all_q_values), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(all_q_values):.1f}')\n",
    "    plt.xlabel('Q-Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('SARSA(λ) Q-Value Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    actions = list(action_dist.keys())\n",
    "    counts = list(action_dist.values())\n",
    "    action_labels = [action_names[a] for a in actions]\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange'][:len(actions)]\n",
    "    plt.bar(action_labels, counts, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Frequency in Policy')\n",
    "    plt.title('SARSA(λ) Learned Policy Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  No Q-table available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-section",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apr.utils import get_outputs_dir\n",
    "\n",
    "# Create save paths\n",
    "outputs_dir = get_outputs_dir()\n",
    "models_dir = outputs_dir / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save trained agent\n",
    "agent_path = models_dir / 'sarsa_lambda_complete.pkl'\n",
    "agent.save(agent_path)\n",
    "print(f\"✅ Saved trained agent to: {agent_path}\")\n",
    "\n",
    "# Save training metrics\n",
    "training_df = pd.DataFrame({\n",
    "    'episode': range(1, training_episodes + 1),\n",
    "    'reward': episode_rewards,\n",
    "    'epsilon': epsilon_values,\n",
    "    'q_table_size': q_table_sizes,\n",
    "    'eligibility_traces': eligibility_trace_sizes,\n",
    "    'episode_length': episode_lengths,\n",
    "    'training_time': training_times\n",
    "})\n",
    "\n",
    "metrics_path = models_dir / 'sarsa_lambda_training_metrics.csv'\n",
    "training_df.to_csv(metrics_path, index=False)\n",
    "print(f\"📊 Saved training metrics to: {metrics_path}\")\n",
    "\n",
    "# Save validation results\n",
    "validation_dir = outputs_dir / 'validation_results'\n",
    "validation_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save test results\n",
    "test_summary = pd.DataFrame([{\n",
    "    'algorithm': 'SARSA(λ)',\n",
    "    'lambda': agent.lambda_,\n",
    "    'mean_reward': agg_stats['mean_reward'],\n",
    "    'std_reward': agg_stats['std_reward'],\n",
    "    'success_rate': agg_stats['mean_success_rate'],\n",
    "    'episode_length': agg_stats['mean_episode_length'],\n",
    "    'state_coverage': agg_stats['mean_state_coverage'],\n",
    "    'training_episodes': training_episodes,\n",
    "    'training_time': total_training_time,\n",
    "    'final_q_table_size': len(agent.Q),\n",
    "    'avg_active_traces': np.mean(eligibility_trace_sizes[-100:])\n",
    "}])\n",
    "\n",
    "test_path = validation_dir / 'sarsa_lambda_test_summary.csv'\n",
    "test_summary.to_csv(test_path, index=False)\n",
    "print(f\"🎯 Saved test summary to: {test_path}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎉 SARSA(λ) COMPLETE WORKFLOW FINISHED!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training Episodes: {training_episodes}\")\n",
    "print(f\"Training Time: {total_training_time:.1f}s\")\n",
    "print(f\"Final Performance: {np.mean(episode_rewards[-50:]):.1f} reward\")\n",
    "print(f\"Test Performance: {agg_stats['mean_reward']:.1f} ± {agg_stats['std_reward']:.1f}\")\n",
    "print(f\"Success Rate: {agg_stats['mean_success_rate']:.1%}\")\n",
    "print(f\"Validation Status: {summary['overall_assessment']}\")\n",
    "print(f\"\\n🔥 SARSA(λ) Advantages:\")\n",
    "print(f\"  • Eligibility traces (λ={agent.lambda_}) for faster learning\")\n",
    "print(f\"  • Multi-step credit assignment\")\n",
    "print(f\"  • On-policy stability with enhanced convergence\")\n",
    "print(f\"  • Average {np.mean(eligibility_trace_sizes[-100:]):.1f} active traces per episode\")\n",
    "print(f\"  • Better than SARSA for delayed reward scenarios\")\n",
    "print(\"\\n✅ All results saved to outputs directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}