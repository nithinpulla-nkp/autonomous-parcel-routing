{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "# Agent Validation Notebook\n",
    "\n",
    "This notebook provides comprehensive validation of trained RL agents to ensure they are actually learning and not just memorizing paths or getting stuck in local optima.\n",
    "\n",
    "## Validation Tests:\n",
    "1. **Learning vs Random**: Statistical significance testing\n",
    "2. **Convergence Analysis**: Stability and improvement over time\n",
    "3. **Exploration Analysis**: State coverage and action diversity\n",
    "4. **Generalization**: Performance across different scenarios\n",
    "5. **Local Optima Detection**: Multiple training runs comparison\n",
    "6. **Policy Analysis**: Learned behavior examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "from apr import WarehouseEnv, RLAgentValidator\n",
    "from apr.agents import create_agent\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup Environment and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = WarehouseEnv(seed=42)\n",
    "print(f\"Environment: {env.n_rows}x{env.n_cols} warehouse\")\n",
    "print(f\"Action space: {env.n_actions} actions\")\n",
    "print(f\"State space: {env.n_rows * env.n_cols} possible states\")\n",
    "\n",
    "# Create agents to validate\n",
    "agents_to_validate = {\n",
    "    'Q-Learning': create_agent('q_learning', env.observation_space, env.action_space, \n",
    "                              alpha=0.1, gamma=0.95, epsilon=0.3),\n",
    "    'Double Q-Learning': create_agent('double_q_learning', env.observation_space, env.action_space, \n",
    "                                     alpha=0.1, gamma=0.95, epsilon=0.3),\n",
    "    'SARSA': create_agent('sarsa', env.observation_space, env.action_space, \n",
    "                         alpha=0.1, gamma=0.95, epsilon=0.3)\n",
    "}\n",
    "\n",
    "print(f\"\\nAgents to validate: {list(agents_to_validate.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-function",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_agent(agent_name, agent, training_episodes=400, test_episodes=50, n_seeds=3):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of a single agent.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Validating {agent_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create validator\n",
    "    validator = RLAgentValidator(agent, env, verbose=True)\n",
    "    \n",
    "    # Run full validation suite\n",
    "    results = validator.full_validation(\n",
    "        training_episodes=training_episodes,\n",
    "        test_episodes=test_episodes,\n",
    "        n_seeds=n_seeds\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-validation",
   "metadata": {},
   "source": [
    "## Individual Agent Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-qlearning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Q-Learning\n",
    "qlearning_results = validate_agent('Q-Learning', agents_to_validate['Q-Learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-double-qlearning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Double Q-Learning\n",
    "double_qlearning_results = validate_agent('Double Q-Learning', agents_to_validate['Double Q-Learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-sarsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate SARSA\n",
    "sarsa_results = validate_agent('SARSA', agents_to_validate['SARSA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Validation Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = {\n",
    "    'Q-Learning': qlearning_results,\n",
    "    'Double Q-Learning': double_qlearning_results,\n",
    "    'SARSA': sarsa_results\n",
    "}\n",
    "\n",
    "# Create comparison summary\n",
    "print(\"\\nüìä VALIDATION COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_data = []\n",
    "for agent_name, results in all_results.items():\n",
    "    summary = results['summary']\n",
    "    learning_result = results['learning']\n",
    "    exploration = results['exploration']\n",
    "    generalization = results['generalization']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Agent': agent_name,\n",
    "        'Overall': summary['overall_assessment'],\n",
    "        'Learning Improvement': f\"{learning_result['improvement']:.1f}\",\n",
    "        'P-Value': f\"{learning_result['statistical_test']['p_value']:.4f}\",\n",
    "        'State Coverage': f\"{exploration['state_coverage']['coverage_percent']:.1f}%\",\n",
    "        'Generalization': f\"{generalization['consistency_score']:.3f}\"\n",
    "    })\n",
    "\n",
    "# Display comparison table\n",
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Count validation outcomes\n",
    "outcomes = [results['summary']['overall_assessment'] for results in all_results.values()]\n",
    "print(f\"\\nüìà Validation Outcomes:\")\n",
    "print(f\"  PASS: {outcomes.count('PASS')}\")\n",
    "print(f\"  PASS_WITH_WARNINGS: {outcomes.count('PASS_WITH_WARNINGS')}\")\n",
    "print(f\"  FAIL: {outcomes.count('FAIL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-analysis",
   "metadata": {},
   "source": [
    "## Detailed Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "agent_names = list(all_results.keys())\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "# 1. Learning improvement comparison\n",
    "ax1 = axes[0, 0]\n",
    "improvements = [all_results[name]['learning']['improvement'] for name in agent_names]\n",
    "bars1 = ax1.bar(agent_names, improvements, color=colors)\n",
    "ax1.set_title('Learning vs Random Performance')\n",
    "ax1.set_ylabel('Reward Improvement')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, improvements):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "             f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. State coverage comparison\n",
    "ax2 = axes[0, 1]\n",
    "coverages = [all_results[name]['exploration']['state_coverage']['coverage_percent'] \n",
    "            for name in agent_names]\n",
    "bars2 = ax2.bar(agent_names, coverages, color=colors)\n",
    "ax2.set_title('State Coverage')\n",
    "ax2.set_ylabel('Coverage %')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, val in zip(bars2, coverages):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{val:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# 3. Generalization consistency\n",
    "ax3 = axes[0, 2]\n",
    "consistencies = [all_results[name]['generalization']['consistency_score'] \n",
    "                for name in agent_names]\n",
    "bars3 = ax3.bar(agent_names, consistencies, color=colors)\n",
    "ax3.set_title('Generalization Consistency')\n",
    "ax3.set_ylabel('Consistency Score')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, val in zip(bars3, consistencies):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Statistical significance\n",
    "ax4 = axes[1, 0]\n",
    "p_values = [all_results[name]['learning']['statistical_test']['p_value'] \n",
    "           for name in agent_names]\n",
    "bars4 = ax4.bar(agent_names, [-np.log10(p) for p in p_values], color=colors)\n",
    "ax4.set_title('Statistical Significance (-log10(p))')\n",
    "ax4.set_ylabel('-log10(p-value)')\n",
    "ax4.axhline(-np.log10(0.05), color='red', linestyle='--', label='p=0.05 threshold')\n",
    "ax4.legend()\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Action diversity\n",
    "ax5 = axes[1, 1]\n",
    "diversities = [all_results[name]['exploration']['action_diversity']['diversity_percent'] \n",
    "              for name in agent_names]\n",
    "bars5 = ax5.bar(agent_names, diversities, color=colors)\n",
    "ax5.set_title('Action Diversity')\n",
    "ax5.set_ylabel('Diversity %')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, val in zip(bars5, diversities):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{val:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# 6. Local optima performance range\n",
    "ax6 = axes[1, 2]\n",
    "ranges = [all_results[name]['local_optima']['performance_range'] \n",
    "         for name in agent_names]\n",
    "bars6 = ax6.bar(agent_names, ranges, color=colors)\n",
    "ax6.set_title('Local Optima (Performance Range)')\n",
    "ax6.set_ylabel('Performance Range')\n",
    "ax6.axhline(50, color='red', linestyle='--', label='Concern threshold')\n",
    "ax6.legend()\n",
    "ax6.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-issues",
   "metadata": {},
   "source": [
    "## Validation Issues and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-issues",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è  VALIDATION ISSUES AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for agent_name, results in all_results.items():\n",
    "    summary = results['summary']\n",
    "    \n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    print(f\"  Overall Assessment: {summary['overall_assessment']}\")\n",
    "    \n",
    "    if summary['critical_issues']:\n",
    "        print(\"  ‚ùå Critical Issues:\")\n",
    "        for issue in summary['critical_issues']:\n",
    "            print(f\"    - {issue}\")\n",
    "    \n",
    "    if summary['warnings']:\n",
    "        print(\"  ‚ö†Ô∏è  Warnings:\")\n",
    "        for warning in summary['warnings']:\n",
    "            print(f\"    - {warning}\")\n",
    "    \n",
    "    if not summary['critical_issues'] and not summary['warnings']:\n",
    "        print(\"  ‚úÖ No issues detected - agent is learning properly!\")\n",
    "\n",
    "# Overall recommendations\n",
    "print(\"\\nüí° OVERALL RECOMMENDATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check for common issues\n",
    "low_coverage_agents = [name for name, results in all_results.items() \n",
    "                      if results['exploration']['state_coverage']['coverage_percent'] < 50]\n",
    "\n",
    "if low_coverage_agents:\n",
    "    print(f\"‚Ä¢ Agents with low state coverage: {', '.join(low_coverage_agents)}\")\n",
    "    print(\"  ‚Üí Consider increasing exploration (higher epsilon) or longer training\")\n",
    "\n",
    "# Check for statistical significance\n",
    "non_significant = [name for name, results in all_results.items() \n",
    "                  if not results['learning']['statistical_test']['significant']]\n",
    "\n",
    "if non_significant:\n",
    "    print(f\"‚Ä¢ Agents without statistically significant learning: {', '.join(non_significant)}\")\n",
    "    print(\"  ‚Üí These agents may not be learning effectively\")\n",
    "\n",
    "# Performance ranking\n",
    "performance_ranking = sorted(all_results.items(), \n",
    "                           key=lambda x: x[1]['learning']['improvement'], reverse=True)\n",
    "\n",
    "print(f\"\\nüèÜ Performance Ranking (by learning improvement):\")\n",
    "for i, (name, results) in enumerate(performance_ranking):\n",
    "    improvement = results['learning']['improvement']\n",
    "    print(f\"  {i+1}. {name}: {improvement:.1f} reward improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-section",
   "metadata": {},
   "source": [
    "## Individual Agent Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the best performing agent in detail\n",
    "best_agent_name = performance_ranking[0][0]\n",
    "best_results = performance_ranking[0][1]\n",
    "\n",
    "print(f\"üîç Detailed visualization for best agent: {best_agent_name}\")\n",
    "\n",
    "# Create validator for visualization\n",
    "best_agent = agents_to_validate[best_agent_name]\n",
    "validator = RLAgentValidator(best_agent, env, verbose=False)\n",
    "validator.validation_results = best_results\n",
    "\n",
    "# Generate comprehensive visualization\n",
    "validator.visualize_results()\n",
    "\n",
    "print(f\"\\n‚úÖ Detailed validation visualization complete for {best_agent_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## Save Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation results\n",
    "results_dir = Path('../validation_results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Save detailed results\n",
    "for agent_name, results in all_results.items():\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    json_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            json_results[key] = {}\n",
    "            for subkey, subvalue in value.items():\n",
    "                if isinstance(subvalue, np.ndarray):\n",
    "                    json_results[key][subkey] = subvalue.tolist()\n",
    "                elif isinstance(subvalue, dict):\n",
    "                    json_results[key][subkey] = subvalue\n",
    "                else:\n",
    "                    json_results[key][subkey] = subvalue\n",
    "        else:\n",
    "            json_results[key] = value\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_path = results_dir / f'{agent_name.lower().replace(\" \", \"_\")}_validation.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved {agent_name} validation results to {json_path}\")\n",
    "\n",
    "# Save comparison summary\n",
    "comparison_path = results_dir / 'validation_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"üìä Saved comparison summary to {comparison_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All validation results saved to {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}