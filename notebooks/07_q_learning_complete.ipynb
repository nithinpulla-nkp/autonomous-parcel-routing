{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Q-Learning: Complete Training, Validation & Testing\n",
    "\n",
    "This notebook provides a complete workflow for Q-Learning agent:\n",
    "1. **Training**: Learn optimal policy through experience\n",
    "2. **Validation**: Rigorous testing to ensure proper learning\n",
    "3. **Testing**: Evaluate final performance and behavior\n",
    "\n",
    "## Q-Learning Algorithm\n",
    "- **Type**: Off-policy temporal difference learning\n",
    "- **Update Rule**: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
    "- **Key Feature**: Uses max over next actions (optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "from apr import WarehouseEnv, RLAgentValidator, AgentEvaluator\n",
    "from apr.agents import create_agent\n",
    "from apr.train import run_episode\n",
    "from apr.utils import ensure_outputs_directory\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "# Ensure output directories exist\n",
    "ensure_outputs_directory()\n",
    "print(\"‚úÖ Output directories ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Environment & Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = WarehouseEnv(seed=42)\n",
    "print(f\"Environment: {env.n_rows}x{env.n_cols} warehouse\")\n",
    "print(f\"Packages to collect: {len(env.packages_remaining)}\")\n",
    "print(f\"Max steps per episode: {env.max_steps}\")\n",
    "\n",
    "# Visualize environment\n",
    "env.reset()\n",
    "env.render(mode='human')\n",
    "plt.title('Q-Learning Training Environment')\n",
    "plt.show()\n",
    "\n",
    "# Create Q-Learning agent\n",
    "agent = create_agent(\n",
    "    'q_learning',\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    alpha=0.1,          # Learning rate\n",
    "    gamma=0.95,         # Discount factor\n",
    "    epsilon=0.3,        # Initial exploration rate\n",
    "    epsilon_decay=0.999, # Exploration decay\n",
    "    epsilon_min=0.05    # Minimum exploration\n",
    ")\n",
    "\n",
    "print(f\"\\nü§ñ Created Q-Learning Agent:\")\n",
    "print(f\"  Algorithm: {type(agent).__name__}\")\n",
    "print(f\"  Learning rate (Œ±): {agent.alpha}\")\n",
    "print(f\"  Discount factor (Œ≥): {agent.gamma}\")\n",
    "print(f\"  Initial exploration (Œµ): {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## 2. Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_episodes = 800\n",
    "log_interval = 100\n",
    "\n",
    "print(\"üöÄ Starting Q-Learning Training\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "epsilon_values = []\n",
    "q_table_sizes = []\n",
    "training_times = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(training_episodes):\n",
    "    episode_start = time.time()\n",
    "    \n",
    "    # Run training episode\n",
    "    reward = run_episode(env, agent, training=True)\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(reward)\n",
    "    episode_lengths.append(env.episode_length if hasattr(env, 'episode_length') else 0)\n",
    "    epsilon_values.append(agent.epsilon)\n",
    "    q_table_sizes.append(len(agent.Q))\n",
    "    training_times.append(time.time() - episode_start)\n",
    "    \n",
    "    # Logging\n",
    "    if (episode + 1) % log_interval == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-log_interval:])\n",
    "        avg_time = np.mean(training_times[-log_interval:])\n",
    "        print(f\"Episode {episode + 1:3d}: Reward = {avg_reward:6.1f}, Œµ = {agent.epsilon:.3f}, \"\n",
    "              f\"Q-states = {len(agent.Q):3d}, Time = {avg_time:.3f}s\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"  Total time: {total_training_time:.1f}s\")\n",
    "print(f\"  Final performance: {np.mean(episode_rewards[-50:]):.1f} (last 50 episodes)\")\n",
    "print(f\"  Q-table size: {len(agent.Q)} states\")\n",
    "print(f\"  Final exploration: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-analysis",
   "metadata": {},
   "source": [
    "## 3. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Learning curve\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='skyblue', linewidth=0.5, label='Raw')\n",
    "window = 50\n",
    "smoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), smoothed, 'b-', linewidth=2, label='Smoothed')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Q-Learning: Learning Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Epsilon decay\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epsilon_values, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "ax2.set_title('Exploration Rate Decay')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-table growth\n",
    "ax3 = axes[0, 2]\n",
    "ax3.plot(q_table_sizes, 'g-', linewidth=2)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Number of States')\n",
    "ax3.set_title('Q-Table Growth')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Episode lengths\n",
    "ax4 = axes[1, 0]\n",
    "lengths_smoothed = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "ax4.plot(range(window-1, len(episode_lengths)), lengths_smoothed, 'm-', linewidth=2)\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Episode Length')\n",
    "ax4.set_title('Episode Length Over Time')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Training time per episode\n",
    "ax5 = axes[1, 1]\n",
    "time_smoothed = np.convolve(training_times, np.ones(window)/window, mode='valid')\n",
    "ax5.plot(range(window-1, len(training_times)), time_smoothed, 'orange', linewidth=2)\n",
    "ax5.set_xlabel('Episode')\n",
    "ax5.set_ylabel('Time per Episode (s)')\n",
    "ax5.set_title('Training Efficiency')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Final reward distribution\n",
    "ax6 = axes[1, 2]\n",
    "final_rewards = episode_rewards[-200:]  # Last 200 episodes\n",
    "ax6.hist(final_rewards, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax6.axvline(np.mean(final_rewards), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(final_rewards):.1f}')\n",
    "ax6.set_xlabel('Reward')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.set_title('Final Performance Distribution')\n",
    "ax6.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print training statistics\n",
    "print(\"üìä Training Statistics:\")\n",
    "print(f\"  Episodes: {training_episodes}\")\n",
    "print(f\"  Total time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)\")\n",
    "print(f\"  Avg time per episode: {np.mean(training_times):.3f}s\")\n",
    "print(f\"  Final 100-episode average: {np.mean(episode_rewards[-100:]):.1f}\")\n",
    "print(f\"  Best single episode: {np.max(episode_rewards):.1f}\")\n",
    "print(f\"  Q-table final size: {len(agent.Q)} states\")\n",
    "print(f\"  State space coverage: {len(agent.Q)/(env.n_rows*env.n_cols)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-section",
   "metadata": {},
   "source": [
    "## 4. Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Q-Learning Agent Validation\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create fresh agent for validation (to avoid training bias)\n",
    "validation_agent = create_agent(\n",
    "    'q_learning',\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    alpha=0.1, gamma=0.95, epsilon=0.3\n",
    ")\n",
    "\n",
    "# Run comprehensive validation\n",
    "validator = RLAgentValidator(validation_agent, env, verbose=True)\n",
    "validation_results = validator.full_validation(\n",
    "    training_episodes=300,\n",
    "    test_episodes=50,\n",
    "    n_seeds=3\n",
    ")\n",
    "\n",
    "# Display validation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = validation_results['summary']\n",
    "print(f\"Overall Assessment: {summary['overall_assessment']}\")\n",
    "print()\n",
    "\n",
    "print(\"Component Scores:\")\n",
    "for component, score in summary['scores'].items():\n",
    "    status_icon = \"‚úÖ\" if score == \"PASS\" else \"‚ö†Ô∏è\" if score == \"WARNING\" else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {component.capitalize()}: {score}\")\n",
    "\n",
    "if summary['warnings']:\n",
    "    print(\"\\n‚ö†Ô∏è  Warnings:\")\n",
    "    for warning in summary['warnings']:\n",
    "        print(f\"  - {warning}\")\n",
    "\n",
    "# Key metrics\n",
    "learning_result = validation_results['learning']\n",
    "exploration = validation_results['exploration']\n",
    "generalization = validation_results['generalization']\n",
    "\n",
    "print(\"\\nKey Validation Metrics:\")\n",
    "print(f\"  Learning improvement: {learning_result['improvement']:.1f} reward vs random\")\n",
    "print(f\"  Statistical significance: p={learning_result['statistical_test']['p_value']:.4f}\")\n",
    "print(f\"  State coverage: {exploration['state_coverage']['coverage_percent']:.1f}%\")\n",
    "print(f\"  Generalization consistency: {generalization['consistency_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-viz",
   "metadata": {},
   "source": [
    "## 5. Validation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive validation visualization\n",
    "validator.visualize_results()\n",
    "print(\"‚úÖ Validation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "## 6. Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Q-Learning Agent Testing\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use the trained agent for testing\n",
    "evaluator = AgentEvaluator(env, verbose=True)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "test_results = evaluator.evaluate_agent(\n",
    "    agent,\n",
    "    num_episodes=100,\n",
    "    seeds=[42, 123, 456, 789, 999],  # Multiple scenarios\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Display test results\n",
    "print(\"\\nüìä TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "agg_stats = test_results['aggregated_results']['overall_statistics']\n",
    "print(f\"Mean Reward: {agg_stats['mean_reward']:.1f} ¬± {agg_stats['std_reward']:.1f}\")\n",
    "print(f\"Success Rate: {agg_stats['mean_success_rate']:.1%}\")\n",
    "print(f\"Mean Episode Length: {agg_stats['mean_episode_length']:.1f}\")\n",
    "print(f\"State Coverage: {agg_stats['mean_state_coverage']:.1%}\")\n",
    "\n",
    "# Performance across different seeds\n",
    "seed_results = test_results['per_seed_results']\n",
    "print(\"\\nPerformance Across Seeds:\")\n",
    "for seed_key, result in seed_results.items():\n",
    "    seed = result['seed']\n",
    "    mean_reward = result['statistics']['mean_reward']\n",
    "    success_rate = result['statistics']['success_rate']\n",
    "    print(f\"  Seed {seed}: {mean_reward:.1f} reward, {success_rate:.1%} success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-viz",
   "metadata": {},
   "source": [
    "## 7. Testing Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing visualization\n",
    "evaluator.visualize_evaluation(test_results)\n",
    "print(\"‚úÖ Testing visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-section",
   "metadata": {},
   "source": [
    "## 8. Agent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé¨ Q-Learning Agent Demonstration\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Reset for demonstration\n",
    "env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "max_demo_steps = 25\n",
    "\n",
    "print(f\"Initial state: Agent at {env.agent_pos}\")\n",
    "print(f\"Packages to collect: {env.packages_remaining}\")\n",
    "print(f\"Dropoff location: {env.dropoff}\")\n",
    "print()\n",
    "\n",
    "# Action names for readability\n",
    "action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "\n",
    "while not done and steps < max_demo_steps:\n",
    "    state = env.agent_pos\n",
    "    action = agent.act(state, training=False)  # No exploration\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {steps + 1:2d}: {state} ‚Üí {action_names[action]:5s} ‚Üí {next_state} \"\n",
    "          f\"(reward: {reward:+4.0f}, total: {total_reward:+4.0f})\")\n",
    "    \n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nDemo completed after {steps} steps\")\n",
    "print(f\"Final reward: {total_reward}\")\n",
    "print(f\"Episode completed: {done}\")\n",
    "print(f\"Packages remaining: {len(env.packages_remaining)}\")\n",
    "print(f\"Carrying packages: {env.carrying_packages}\")\n",
    "\n",
    "# Show final state\n",
    "env.render(mode='human')\n",
    "plt.title(f'Q-Learning Agent After {steps} Steps (Reward: {total_reward})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "policy-analysis",
   "metadata": {},
   "source": [
    "## 9. Policy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "policy-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Q-Learning Policy Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Analyze learned Q-values and policy\n",
    "if hasattr(agent, 'Q') and len(agent.Q) > 0:\n",
    "    # Extract Q-values and policy\n",
    "    all_q_values = []\n",
    "    policy = {}\n",
    "    state_values = {}\n",
    "    \n",
    "    for state, q_vals in agent.Q.items():\n",
    "        if isinstance(q_vals, np.ndarray):\n",
    "            all_q_values.extend(q_vals)\n",
    "            policy[state] = np.argmax(q_vals)\n",
    "            state_values[state] = np.max(q_vals)\n",
    "    \n",
    "    print(f\"Q-table Statistics:\")\n",
    "    print(f\"  States learned: {len(agent.Q)}\")\n",
    "    print(f\"  Q-value range: [{np.min(all_q_values):.1f}, {np.max(all_q_values):.1f}]\")\n",
    "    print(f\"  Q-value mean: {np.mean(all_q_values):.1f}\")\n",
    "    print(f\"  Q-value std: {np.std(all_q_values):.1f}\")\n",
    "    \n",
    "    # Action distribution in policy\n",
    "    from collections import Counter\n",
    "    action_dist = Counter(policy.values())\n",
    "    print(f\"\\nPolicy Action Distribution:\")\n",
    "    for action, count in action_dist.items():\n",
    "        percentage = count / len(policy) * 100\n",
    "        print(f\"  {action_names[action]:5s}: {count:3d} states ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # Visualize Q-value distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(all_q_values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(np.mean(all_q_values), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(all_q_values):.1f}')\n",
    "    plt.xlabel('Q-Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Q-Value Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    actions = list(action_dist.keys())\n",
    "    counts = list(action_dist.values())\n",
    "    action_labels = [action_names[a] for a in actions]\n",
    "    \n",
    "    plt.bar(action_labels, counts, color=['red', 'blue', 'green', 'orange'][:len(actions)], alpha=0.7)\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Frequency in Policy')\n",
    "    plt.title('Learned Policy Action Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No Q-table available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-section",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apr.utils import get_outputs_dir\n",
    "\n",
    "# Create save paths\n",
    "outputs_dir = get_outputs_dir()\n",
    "models_dir = outputs_dir / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save trained agent\n",
    "agent_path = models_dir / 'q_learning_complete.pkl'\n",
    "agent.save(agent_path)\n",
    "print(f\"‚úÖ Saved trained agent to: {agent_path}\")\n",
    "\n",
    "# Save training metrics\n",
    "training_df = pd.DataFrame({\n",
    "    'episode': range(1, training_episodes + 1),\n",
    "    'reward': episode_rewards,\n",
    "    'epsilon': epsilon_values,\n",
    "    'q_table_size': q_table_sizes,\n",
    "    'episode_length': episode_lengths,\n",
    "    'training_time': training_times\n",
    "})\n",
    "\n",
    "metrics_path = models_dir / 'q_learning_training_metrics.csv'\n",
    "training_df.to_csv(metrics_path, index=False)\n",
    "print(f\"üìä Saved training metrics to: {metrics_path}\")\n",
    "\n",
    "# Save validation results\n",
    "validation_dir = outputs_dir / 'validation_results'\n",
    "validation_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save test results\n",
    "test_summary = pd.DataFrame([{\n",
    "    'algorithm': 'Q-Learning',\n",
    "    'mean_reward': agg_stats['mean_reward'],\n",
    "    'std_reward': agg_stats['std_reward'],\n",
    "    'success_rate': agg_stats['mean_success_rate'],\n",
    "    'episode_length': agg_stats['mean_episode_length'],\n",
    "    'state_coverage': agg_stats['mean_state_coverage'],\n",
    "    'training_episodes': training_episodes,\n",
    "    'training_time': total_training_time,\n",
    "    'final_q_table_size': len(agent.Q)\n",
    "}])\n",
    "\n",
    "test_path = validation_dir / 'q_learning_test_summary.csv'\n",
    "test_summary.to_csv(test_path, index=False)\n",
    "print(f\"üéØ Saved test summary to: {test_path}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ Q-LEARNING COMPLETE WORKFLOW FINISHED!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training Episodes: {training_episodes}\")\n",
    "print(f\"Training Time: {total_training_time:.1f}s\")\n",
    "print(f\"Final Performance: {np.mean(episode_rewards[-50:]):.1f} reward\")\n",
    "print(f\"Test Performance: {agg_stats['mean_reward']:.1f} ¬± {agg_stats['std_reward']:.1f}\")\n",
    "print(f\"Success Rate: {agg_stats['mean_success_rate']:.1%}\")\n",
    "print(f\"Validation Status: {summary['overall_assessment']}\")\n",
    "print(\"\\n‚úÖ All results saved to outputs directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}