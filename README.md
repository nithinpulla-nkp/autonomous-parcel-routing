# Autonomous Parcel Routing

> **A comprehensive reinforcement learning framework for warehouse automation**  
> Train intelligent agents to navigate complex warehouse environments, collect packages, and optimize delivery routes using multiple RL algorithms.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Overview

**Autonomous Parcel Routing** is a modern reinforcement learning research platform that simulates warehouse automation challenges. Agents learn to navigate gridworld environments with obstacles, collect packages, and deliver them efficiently while maximizing reward and minimizing collisions.

### Key Features

- ğŸ¤– **4 RL Algorithms**: Q-Learning, Double Q-Learning, SARSA, SARSA(Î») with eligibility traces
- ğŸ­ **Realistic Warehouse Environment**: Package pickup/delivery with obstacles and sprites
- ğŸ“Š **Comprehensive Validation**: Statistical testing ensures agents actually learn (not memorize)
- ğŸ““ **Complete Notebooks**: Individual training/validation/testing workflows for each algorithm
- ğŸ¨ **Rich Visualizations**: Training curves, policy analysis, performance comparisons
- ğŸ”¬ **Professional Evaluation**: Multi-seed testing, generalization analysis, local optima detection

## ğŸš€ Quick Start

```bash
# 1. Clone and setup
git clone <repository-url>
cd autonomous-parcel-routing

# 2. Create virtual environment
python3 -m venv .apr_venv
source .apr_venv/bin/activate  # Windows: .apr_venv\Scripts\activate

# 3. Install package
pip install -e .

# 4. Verify installation
python verify_setup.py

# 5. Train your first agent
python src/apr/train.py --config cfg/baseline.yaml

# 6. Explore interactive notebooks
cd notebooks && jupyter notebook
```

## ğŸ—ï¸ Architecture

```
autonomous-parcel-routing/
â”œâ”€â”€ src/apr/                    # Core RL framework
â”‚   â”œâ”€â”€ agents/                # Algorithm implementations
â”‚   â”‚   â”œâ”€â”€ q_learning.py      # Classic Q-Learning
â”‚   â”‚   â”œâ”€â”€ double_q_learning.py # Overestimation bias reduction
â”‚   â”‚   â”œâ”€â”€ sarsa.py           # On-policy SARSA + SARSA(Î»)
â”‚   â”‚   â””â”€â”€ registry.py        # Agent factory system
â”‚   â”œâ”€â”€ env.py                 # Warehouse environment
â”‚   â”œâ”€â”€ validation.py          # Statistical validation framework
â”‚   â”œâ”€â”€ evaluate.py            # Comprehensive evaluation tools
â”‚   â””â”€â”€ train.py               # Training pipeline
â”œâ”€â”€ notebooks/                 # Interactive exploration
â”‚   â”œâ”€â”€ 07_q_learning_complete.ipynb      # Q-Learning workflow
â”‚   â”œâ”€â”€ 08_sarsa_complete.ipynb           # SARSA workflow  
â”‚   â”œâ”€â”€ 09_sarsa_lambda_complete.ipynb    # SARSA(Î») workflow
â”‚   â”œâ”€â”€ 05_agent_validation.ipynb         # Multi-agent validation
â”‚   â””â”€â”€ 06_agent_comparison.ipynb         # Algorithm comparison
â”œâ”€â”€ cfg/                       # Configuration files
â”‚   â”œâ”€â”€ baseline.yaml          # Default Q-Learning config
â”‚   â”œâ”€â”€ double_q_learning.yaml # Double Q-Learning config
â”‚   â””â”€â”€ sarsa.yaml            # SARSA config
â””â”€â”€ outputs/                   # Training results (auto-created)
    â”œâ”€â”€ runs/                  # Training metrics & checkpoints
    â”œâ”€â”€ models/               # Saved agents
    â”œâ”€â”€ validation_results/   # Validation reports
    â””â”€â”€ comparison_results/   # Algorithm comparisons
```

## ğŸ¤– Implemented Algorithms

| Algorithm | Type | Key Features | Notebook |
|-----------|------|--------------|----------|
| **Q-Learning** | Off-policy TD | Fast convergence, optimistic updates | `07_q_learning_complete.ipynb` |
| **Double Q-Learning** | Off-policy TD | Reduces overestimation bias, dual Q-tables | `04_double_q_learning_training.ipynb` |
| **SARSA** | On-policy TD | Conservative, safer exploration | `08_sarsa_complete.ipynb` |
| **SARSA(Î»)** | On-policy TD + traces | Faster credit assignment, eligibility traces | `09_sarsa_lambda_complete.ipynb` |

## ğŸ­ Environment Features

- **Grid-based warehouse** with configurable size and obstacles
- **Package pickup/delivery mechanics** with inventory tracking
- **Rich reward structure**: +25 pickup, +100 delivery, -20 collision
- **Multiple rendering modes**: ASCII, sprites, human-readable
- **Gymnasium-compatible interface** for easy integration

## ğŸ“Š Validation & Testing

Our framework includes rigorous validation to ensure agents actually learn:

- **Statistical significance testing** vs random baseline
- **Convergence analysis** with Q-value and policy tracking  
- **Exploration analysis** measuring state coverage and action diversity
- **Generalization testing** across multiple random seeds
- **Local optima detection** through multiple training runs
- **Policy analysis** of learned behavior patterns

## ğŸ“ˆ Performance Results

Recent validation results show all algorithms successfully learn:

| Algorithm | Reward Improvement | Success Rate | State Coverage | Status |
|-----------|-------------------|--------------|----------------|--------|
| Q-Learning | +817.2 vs random | 95%+ | 72.2% | âœ… PASS |
| Double Q-Learning | +989.7 vs random | 98%+ | 63.9% | âœ… PASS |
| SARSA | +983.5 vs random | 90%+ | 33.3% | âš ï¸ PASS_WITH_WARNINGS |

## ğŸ® Usage Examples

### Command Line Training
```bash
# Train Q-Learning agent
python src/apr/train.py --config cfg/baseline.yaml

# Train Double Q-Learning 
python src/apr/train.py --config cfg/double_q_learning.yaml

# Validate any algorithm
python src/validate_agent.py --algorithm q_learning --episodes 300

# Compare all algorithms
python src/compare_algorithms.py
```

### Interactive Notebooks
```python
# Quick environment test
from apr import WarehouseEnv
env = WarehouseEnv()
env.reset()
env.render(mode='human')  # Visual rendering

# Train and evaluate agent
from apr.agents import create_agent
from apr.validation import RLAgentValidator

agent = create_agent('q_learning', env.observation_space, env.action_space)
validator = RLAgentValidator(agent, env)
results = validator.full_validation(training_episodes=500)
```

### Programmatic Usage
```python
# Complete training workflow
from apr import WarehouseEnv, AgentEvaluator
from apr.agents import create_agent

# Setup
env = WarehouseEnv(seed=42)
agent = create_agent('double_q_learning', env.observation_space, env.action_space)

# Train (see notebooks for complete implementation)
# ... training loop ...

# Evaluate
evaluator = AgentEvaluator(env)
results = evaluator.evaluate_agent(agent, num_episodes=100)
print(f"Performance: {results['aggregated_results']['overall_statistics']}")
```

## ğŸ“– Documentation

- **[SETUP.md](SETUP.md)** - Detailed setup and installation guide
- **[USAGE.md](USAGE.md)** - Complete usage guide with examples and customization
- **Notebooks** - Interactive tutorials and complete workflows
- **Source code** - Well-documented implementation in `src/apr/`

## ğŸ”¬ Research Applications

This framework is designed for:

- **Algorithm development** - Test new RL algorithms in controlled environments
- **Hyperparameter optimization** - Systematic parameter tuning with statistical validation
- **Benchmark studies** - Fair comparison of multiple algorithms
- **Educational use** - Learn RL concepts through hands-on implementation
- **Warehouse automation research** - Realistic logistics simulation

## ğŸ¤ Contributing

We welcome contributions! Areas for improvement:

- **New algorithms** - PPO, A2C, DQN implementations
- **Environment extensions** - Multi-agent scenarios, dynamic obstacles
- **Visualization enhancements** - Interactive policy visualization
- **Performance optimizations** - Vectorized environments, GPU acceleration

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built on **Gymnasium** for standardized RL environments
- Uses **PyTorch** for neural network components  
- Visualization powered by **Matplotlib** and **Seaborn**
- Statistical analysis with **SciPy**

## ğŸ“ Support

- **Issues**: Report bugs or request features via GitHub Issues
- **Questions**: Check the documentation in `SETUP.md` and `USAGE.md`
- **Examples**: Explore the notebooks in `notebooks/` directory

---

**Ready to get started?** Follow the [setup guide](SETUP.md) or jump into the [usage documentation](USAGE.md)!